---
title: "HW week 12"
author: "w203 teaching team"
subtitle: 'w203: Statistics for Data Science'

output:
  pdf_document: default
  html_document: default
---

```{r load packages, message = FALSE}
library(tidyverse)
library(ggplot2) 

library(sandwich)
library(stargazer)
library(lmtest)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r source functions from project, echo = FALSE}
source('./src/load_and_clean.R')
source('./src/get_robust_se.R')
```

```{r load data} 
d <- load_and_clean(input = 'videos.txt')
```
# Regression analysis of YouTube dataset

You want to explain how much the quality of a video affects the number of views it receives on social media. In a world where people can now buy followers and likes, would such an investment increase the number of views that their content receives?  **This is a causal question.** 

You will use a dataset created by Cheng, Dale and Liu at Simon Fraser University.  It includes observations about 9618 videos shared on YouTube.  Please see [this link](http://netsg.cs.sfu.ca/youtubedata/) for details about how the data was collected.

You will use the following variables:

- `views`: the number of views by YouTube users.
- `average_rating`: This is the average of the ratings that the video received, it is a renamed feature from `rate` that is provided in the original dataset. (Notice that this is different from `cout_of_ratings` which is a count of the total number of ratings that a video has received. 
- `length:` the duration of the video in seconds.

a. Perform a brief exploratory data analysis on the data to discover patterns, outliers, or wrong data entries and summarize your findings.

```{r conduct EDA in this chunk}
d <- d[!(is.na(d$length) | is.na(d$average_rating) | is.na(d$length)),]

#less also remove 0 rating
d <- d[!d$average_rating == 0,]

NROW(d$views[d$views == 0])
NROW(d$average_rating[d$average_rating == 0])
NROW(d$length[d$length == 0])

hist(d$views)
# hist(log(d$views))
hist(d$average_rating)
hist(d$length)

d_views_majority <- d[d$views <= 10000,]
hist(d_views_majority$views)

d_views_middle <- d[d$views > 10000 & d$views <= 100000,]
hist(d_views_middle$views)

d_views_minority <- d[d$views >= 100000,]
hist(d_views_minority$views)

d_length_majority <- d[d$length <= 800,]
hist(d_length_majority$length)

d_length_outliers <- d[d$length > 800,]
hist(d_length_outliers$length)


```

> 'What did you learn from your EDA? Cut this quoted text and describe your analysis in the quote block.'
For Views:
Hightly skewed to low number of views. Most videos have low number of views.
For Average Rating:
Hightly skewed to 5 rating, also there is quite a lot of videos with 0 rating.
For Length:
Hightly skewed to short length video. Most videos are less than 800 seconds.


b. Based on your EDA, select an appropriate variable transformation (if any) to apply to each of your three variables.  You will fit a model of the type,

$$
  f(\text{views}) = \beta_0 + \beta_1 g(\text{rate})  + \beta_3 h(\text{length})
$$ 

Where $f$, $g$ and $h$ are sensible transformations, which might include making *no* transformation. 

```{r fit a regression model here}

ggplot(d, aes(x = average_rating, y = log(views))) + 
  geom_point() + stat_smooth()

ggplot(d, aes(x = log(length), y = log(views))) + 
  geom_point() + stat_smooth()

model <- lm(log(views) ~ average_rating + log(length), data=d)

stargazer(
  model,
  type = 'text',
  se = list(get_robust_se(model))
  )
```



c. Using diagnostic plots, background knowledge, and statistical tests, assess all five assumptions of the CLM. When an assumption is violated, state what response you will take.  As part of this process, you should decide what transformation (if any) to apply to each variable. Iterate against your model until your satisfied that at least four of the five assumption have been reasonably addressed. 

> 1. **IID Data:** 
Based on the description of the main page for this data set, this data is generated by first look at the videos in the categories: "Recently Featured", "Most Viewed", "Top Rated" and "Most Discussed". And from there, a crawler will do breath-first search to look for videos related to the first set of videos found, and continue the same process to build the directed graph of these vidoes. It violates the 'independent' assumpton, since for example videos at one level of the graph are dependent on their parent level. Videos in this graph are somehow related, determined by the YouTube recommendation algorithm. For the identically-distributed assumption, it is also hard to say since we don't exactly know the YouTube algorithm on how they determine the videos to be related to the parent video, and it is highly unlikely a random process so it won't be normally distributed for sure.

> 2. **No Perfect Colinearity:**
This assumption should be satisfied since the average_rating and length are two very different concepts based on domain knowledge. Also based on the model, those two variables are kept by the lm function which should be able to recognize variables with perfect colinearity.

> 3. **Linear Conditional Expectation:**
We are looking for whether the residuals have a linear relationship with the predicted values. Also, the expectation of the residuals is 0.
In our residuals vs. predicted plot, we observed that the fitted line is a close to a straight line, and it lies on the value 0. 
This means that our model satisfies the linear conditional expectation.

```{r code and plots assessing linear conditional expectation}

d %>%
  ggplot(aes(x = predict(model), y = resid(model))) +
  geom_point() + stat_smooth()

```

> 4. **Homoskedastic Errors:** 
We are looking for if the error variance across the whole range of predict values are constant. In the plot, the band around the fitted line represents the variance of the errors against the predicted value. We can see that in the middle portion where all the data are concentrated, the band is realy close to the fitted line and is hard to tell. At both left and right ends of the plot, the band starts fanned out due to lack of data. Using 'bptest' on the model, it gives a small P-value (0.0064), which is less than 0.05 and is in the rejection zone. So we have evidence that errors are not homeskedastic.

```{r code and plots assessing error variance}

bptest(model)

```

> 5. **Normally Distributed Errors:** 
The Q-Q plot shows that most of the points in the middle are close to the fitted line, which means that the residuals are matching closly to the normal distribution in the middle section, and only starts deviating on both tails. The shape of the Q-Q plots indicates that our errords distribution has tails thinner than the normally distributed.

```{r code and plots assessing normally distributed errors}

qqnorm(resid(model), pch = 1, frame = FALSE)
qqline(resid(model), col = "steelblue", lwd = 2)

```