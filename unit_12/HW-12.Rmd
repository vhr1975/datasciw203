---
title: "HW week 12"
author: "w203 teaching team"
subtitle: 'w203: Statistics for Data Science'
output:
  pdf_document: default
  html_document: default
---

```{r load packages, message = FALSE}
library(tidyverse)
library(ggplot2) 

library(sandwich)
library(stargazer)

library(lmtest)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r source functions from project, echo = FALSE}
source('./src/load_and_clean.R')
source('./src/get_robust_se.R')
```

```{r load data} 
d <- load_and_clean(input = 'videos.txt')
```
# Regression analysis of YouTube dataset

You want to explain how much the quality of a video affects the number of views it receives on social media. In a world where people can now buy followers and likes, would such an investment increase the number of views that their content receives?  **This is a causal question.** 

You will use a dataset created by Cheng, Dale and Liu at Simon Fraser University.  It includes observations about 9618 videos shared on YouTube.  Please see [this link](http://netsg.cs.sfu.ca/youtubedata/) for details about how the data was collected.

You will use the following variables:

- `views`: the number of views by YouTube users.
- `average_rating`: This is the average of the ratings that the video received, it is a renamed feature from `rate` that is provided in the original dataset. (Notice that this is different from `cout_of_ratings` which is a count of the total number of ratings that a video has received. 
- `length:` the duration of the video in seconds.

a. Perform a brief exploratory data analysis on the data to discover patterns, outliers, or wrong data entries and summarize your findings.

```{r conduct EDA in this chunk}
# view col names
names(d)
# view data
# View(d)

# basic pre EDA histograms of ratings
hist(d$average_rating)
hist(d$log_of_average_rating)
hist(d$length)
hist(d$views)

# remove all invalid data
# remove 0 average ratings
d <- d[!d$average_rating == 0,]

# remove 0 length
d <- d[!is.na(d$length),]

# remove 0 views
d <- d[!is.na(d$views),]

# View(d)

# histogram plots
ggplot(d) +
  aes(x = average_rating) +
  geom_histogram(bins = 30L, fill = "red") +
  theme_minimal()

ggplot(d) +
  aes(x = length) +
  geom_histogram(bins = 30L, fill = "blue") +
  theme_minimal()

ggplot(d) +
  aes(x = views) +
  geom_histogram(bins = 30L, fill = "green") +
  theme_minimal()

# boxplots
ggplot(d) +
  aes(y = average_rating) +
  geom_boxplot(fill = "orange") +
  theme_minimal()

ggplot(d) +
  aes(y = length) +
  geom_boxplot(fill = "yellow") +
  theme_minimal()

ggplot(d) +
  aes(y = views) +
  geom_boxplot(fill = "cyan") +
  theme_minimal()
# 
# # potential outliers
# # extract the values of the potential outliers based on the IQR criterion using the boxplot.stats()$out
# out <- boxplot.stats(d$average_rating)$out
# # Use the which() function to extract the row number corresponding to these outliers
# out_ind <- which(d$average_rating %in% c(out))
# out_ind
# # go back to the specific rows in the dataset to verify them, or print all variables for these outliers:
# d[out_ind, ]
# 
# out <- boxplot.stats(d$length)$out
# out_ind <- which(d$length %in% c(out))
# out_ind
# 
# d[out_ind, ]
# 
# boxplot.stats(d$views)$out
# out <- boxplot.stats(d$views)$out
# out_ind <- which(d$views %in% c(out))
# out_ind
# 
# d[out_ind, ]

# scatter plots
ggplot(d, aes(x = average_rating, y = views) ) + 
  geom_point() + 
  geom_point(col='pink') + 
  geom_smooth(method = lm, se = FALSE) +
  geom_density2d(col = 'blue')

ggplot(d, aes(x = length, y = views) ) + 
  geom_point() + 
  geom_point(col='pink') + 
  geom_smooth(method = lm, se = FALSE) +
  geom_density2d(col = 'blue')

ggplot(d, aes(x = average_rating, y = log(views) ) ) + 
  geom_point() + 
  geom_point(col='pink') + 
  geom_smooth(method = lm, se = FALSE) +
  geom_density2d(col = 'blue')

ggplot(d, aes(x = log(length), y = log(views) ) ) + 
  geom_point() + 
  geom_point(col='pink') + 
  geom_smooth(method = lm, se = FALSE) +
  geom_density2d(col = 'blue')


```

> 'What did you learn from your EDA? Cut this quoted text and describe your analysis in the quote block.'
Had to remove NA values and zereo values from the data. There are outliers based on the histograms and boxplots created. The average rating has some very low outliers, the length data some high outliers, and the views and higly concentrated on the low end values. 

b. Based on your EDA, select an appropriate variable transformation (if any) to apply to each of your three variables.  You will fit a model of the type,

$$
  f(\text{views}) = \beta_0 + \beta_1 g(\text{rate})  + \beta_3 h(\text{length})
$$ 

Where $f$, $g$ and $h$ are sensible transformations, which might include making *no* transformation. 

```{r fit a regression model here}


model <-lm(log(views) ~ average_rating + log(length), data = d)
model_two <-lm(views ~ average_rating + length, data = d)

summary(model)
summary(model_two)

stargazer(
 model, 
 type = 'text', 
 se = list(get_robust_se(model))
 )
```



c. Using diagnostic plots, background knowledge, and statistical tests, assess all five assumptions of the CLM. When an assumption is violated, state what response you will take.  As part of this process, you should decide what transformation (if any) to apply to each variable. Iterate against your model until your satisfied that at least four of the five assumption have been reasonably addressed. 

> 1. **IID Data:** 
The data is gather by the reaerch the team based on predefined catagories of: "Recently Featured", "Most Viewed", "Top Rated" and "Most Discussed".
Then there is a search / crawl based on the same related catagories. This leads to NOT an independant assumption, as the catagories are related. The data is gathered at different points in time so the identical assumption is met. 


> 2. **No Perfect Colinearity:** 
There is no perfect colineraty between the features of views and length. This is tested with the buitlin lm() function. We can also look at our coefficients, and notice that R has not dropped any variables.

```{r}
model$coefficients
```


> 3. **Linear Conditional Expectation:** 
To assess whether there is a linear conditional expectation, we've learned to look at the predicted vs. residuals of the model. We can see that the we have a more linear pattern in this plot.


```{r code and plots assessing linear conditional expectation}

d %>% 
  mutate(
    model_one_preds = predict(model), 
    model_one_resids = resid(model)
  ) %>% 
  ggplot(aes(model_one_preds, model_one_resids)) + 
  geom_point() + 
  stat_smooth()

```
> 4. **Homoskedastic Errors:** 
we can examine the residuals versus fitted plot again. We can examine the scale-location plot, Homoskedasticity would show up on this plot as a flat smoothing curve. We can also use the Breusch-Pagan test. From the output we can see that the p-value of the test is 0.006429. Since this p-value is less than 0.05, we reject the null hypothesis. We do have sufficient evidence to say that heteroscedasticity is present in the regression model.

```{r code and plots assessing error variance}
bptest(model)
plot(model)
```

> 5. **Normally Distributed Errors:** The qq plot shows specific ways in which the data deviate from normality. The Q-Q plot shows that most of the points in the middle are close to the fitted line, which means that the residuals are matching closly to the normal distribution in the middle section, and only starts deviating on both tails. The shape of the Q-Q plots indicates that our errords distribution has tails thinner than the normally distributed.

```{r code and plots assessing normally distributed errors}

plot(model, which=2)

```